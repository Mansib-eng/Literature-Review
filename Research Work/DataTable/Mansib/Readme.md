# Systematic Literature Review (SLR) Summary: AI-Powered Attention Assessment

| Paper Title                                                                       | AI Techniques Used                        | Dataset Description                                                                                                                                           | Evaluation Metrics               | Reported Accuracy / Effectiveness                                         | Key Challenges Noted                                                                                  | Future Directions Suggested                                                                                       |
|-----------------------------------------------------------------------------------|-------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| *Real-Time Attention Monitoring System for Classroom: A Deep Learning Approach*   | YOLOv5 (v5n, v5s, v5m, v5l, v5x), DeepSORT | - **Action Dataset**: 5,701 images (self-collected + web-sourced) labeled into 9 behaviors. <br> - **Emotion Dataset**: 35,000 images (AffectNet + masked variants). | Precision, Recall, mAP@0.5, F1-score | - **Action Detection**: 76% mAP@0.5 <br> - **Emotion Recognition**: 87.7% mAP@0.5 | - Small dataset size <br> - Limited real-world validation <br> - Privacy concerns <br> - Computational cost | - Integration with social robots <br> - Larger-scale trials <br> - Multi-modal fusion (EEG + vision) <br> - Explainable AI (XAI) |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| *Student-Engagement Detection in Classroom Using Machine Learning Algorithm* | - CATBoost<br>- XGBoost<br>- LightGBM<br>- Random Forest<br>- Multilayer Perceptron | Open University Learning Analytics Dataset (OULAD):<br>- 32,593 student records<br>- 13 features (demographics, VLE interactions, etc.)<br>- Binary engagement classification (high/low) | - Accuracy<br>- Precision<br>- Recall<br>- F1-score<br>- AUC-ROC<br>- Log Loss | - CATBoost: 92.23% accuracy, 94.64% precision, 93.3% recall, 0.9626 AUC<br>- LightGBM: 92.22% accuracy, 94.4% precision<br>- XGBoost: 92.1% accuracy<br>- Outperformed AISAR model (91% precision) | - Class imbalance (72% low engagement)<br>- Limited interpretability of complex models<br>- Generalizability across different learning environments<br>- Privacy concerns with student data | - Incorporate additional data sources (learning preferences, material characteristics)<br>- Investigate dropout-engagement correlation<br>- Adaptive learning interventions<br>- Explainable AI for model transparency |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| *Dyslexia Adaptive Learning Model: Student Engagement Prediction Using Machine Learning Approach* | - Bag of Features (BOF) model<br>- SURF keypoint descriptor<br>- k-Means clustering<br>- SVM (Linear/RBF kernels)<br>- Naïve Bayes<br>- k-NN | - 600 frontal face images (300 engaged/300 disengaged)<br>- Collected from 30 dyslexic students (7-12 years old)<br>- Labeled by dyslexia education experts | - Accuracy<br>- Confusion matrix (TP/TN/FP/FN) | - 97.8% accuracy (SVM Linear)<br>- 97.3% accuracy (Naïve Bayes)<br>- 97.1% accuracy (SVM RBF)<br>- k-NN performed poorly (~60-77%) | - Limited gesture variations in dataset<br>- Partial face occlusion challenges<br>- Small sample size (30 students)<br>- Difficulty classifying ambiguous states (e.g., yawning) | - Integration with adaptive learning systems for dyslexia<br>- Expansion to dynamic video analysis<br>- Inclusion of more behavioral cues (e.g., head pose)<br>- Larger-scale validation across diverse dyslexic populations |

### 17 no PDF found

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| Machine Learning in ADHD and Depression Mental Health Diagnosis: A Survey | **SVM, CNN, Neural Networks, Random Forest, k-NN, Decision Trees, Hybrid Models** | **ADHD-200**: fMRI/EEG (973 participants)<br>**DAIC-WOZ**: Audio/video interviews (142 subjects)<br>**AVEC**: Depression datasets<br>EEG datasets (various sizes) | Accuracy<br>Sensitivity<br>Specificity<br>RMSE<br>AUC | **ADHD**:<br>- 99.58% (EEG)<br>- 97.6% (fMRI)<br>**Depression**:<br>- 100% (EEG)<br>- 89% (clinical notes) | - Small datasets<br>- Data imbalance<br>- Privacy concerns<br>- Generalizability<br>- Subjectivity in labels | - Larger multimodal datasets<br>- Real-time applications<br>- Explainable AI<br>- Clinical workflow integration |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|--------------------|---------------------|--------------------|----------------------------------|----------------------|-----------------------------|
| Machine learning in attention-deficit/hyperactivity disorder: new approaches toward understanding the neural mechanisms | SVM, Random Forest, LDA, Deep Neural Networks, Gaussian Process Classifier, LASSO Regression, Elastic Net | ADHD-200 dataset, ABCD dataset, EEG data, fMRI, sMRI, genetic data | Accuracy, AUC, Sensitivity, Specificity, Mean Square Error, Correlation | 60-90% accuracy for classification models; AUC 60-90% | Small sample sizes, interpretability limitations, generalization issues, feature selection bias | Generative models, dimensional approaches, multi-modal data integration, large-scale datasets |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|--------------------|---------------------|--------------------|----------------------------------|----------------------|-----------------------------|
| Automatic Diagnosis of Attention Deficit Hyperactivity Disorder Using Machine Learning | Decision Tree, Random Forest, SVM, Logistic Regression, Naive Bayes, KNN | NHS clinical data (69 patients) including: <br>- Conner's ADHD Rating Scales <br>- QBTest results <br>- DIVA diagnostic interviews <br>- Risk assessment data <br>- Medical notes | Accuracy, AUC | Best model (Decision Tree): <br>- 85.5% accuracy <br>- 0.871 AUC | - Small sample size (n=69) <br>- Overfitting with medical notes <br>- Interpretability vs accuracy tradeoff <br>- High misclassification cost | - Collect larger datasets <br>- Develop fuzzy rule-based models <br>- Create clinical decision support tool <br>- Implement confidence scoring <br>- Advanced feature selection methods |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|--------------------|---------------------|--------------------|----------------------------------|----------------------|-----------------------------|
| A Machine Learning-Based Analysis of Game Data for ADHD Assessment | AdaBoost, JRip, J48, RandomForest | "Groundskeeper" game data from Sifteo Cubes (52 subjects: 26 ADHD, 26 controls) capturing: <br>- 33 behavioral variables at 10Hz <br>- Tilt/movement metrics <br>- Response timing <br>- Distraction interactions | F-measure (harmonic mean of precision/recall), P-values | - ADHD inattentive type: 78% <br>- ADHD combined type: 75% <br>- Anxiety: 71% <br>- Depression: 76% | - Small sample size (n=52) <br>- High comorbidity rates in sample <br>- Game hardware limitations <br>- Feature engineering complexity | - Larger-scale validation studies <br>- Integration with neuroplasticity therapies <br>- Enhanced sensor capabilities <br>- ASD-specific modeling <br>- Treatment response prediction |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|-------------------|--------------------|-------------------|----------------------------------|----------------------|----------------------------|
| Machine learning-enabled detection of attention-deficit/hyperactivity disorder with multimodal physiological data: a case-control study | Logistic Regression, KNN, Random Forests, SVM | 76 adult participants (32 ADHD patients, 44 healthy controls) with physiological data (EDA, HRV, Skin Temperature) collected during Stroop tests | Accuracy, Sensitivity, Specificity | SVM achieved 81.6% accuracy, 81.4% sensitivity, 81.9% specificity | Small sample size, in-sample validation only, no control for comorbidities/medications | Larger datasets, out-of-sample validation, subtype differentiation, additional data modalities (e.g., performance metrics) |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|-------------------|--------------------|-------------------|----------------------------------|----------------------|----------------------------|
| Assessment of the Autism Spectrum Disorder Based on Machine Learning and Social Visual Attention: A Systematic Review | SVM, ANN (including LSTM, CNN), RF, Naive Bayes, kNN | Eye-tracking data from children with ASD and TD (ages 2-10) viewing social stimuli (static/dynamic) | Accuracy, Sensitivity, Specificity, AUC, Cohen’s Kappa | - SVM: Up to 93.7% accuracy (Li et al., 2018) <br> - ANN: 92.6% accuracy (Li et al., 2020) <br> - Multimodal (EEG + eye-tracking): 85.44% accuracy (Kang et al., 2020) | - Small sample sizes <br> - Lack of ecological validity <br> - Imbalanced datasets <br> - Indirect EM measurement in some studies | - Larger, balanced datasets <br> - Multimodal approaches (e.g., EEG + eye-tracking) <br> - Integration of VR for ecological validity <br> - Unsupervised ML for stratification |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|-------------------|--------------------|-------------------|----------------------------------|----------------------|----------------------------|
| **A systematic review on the application of machine learning models in psychometric questionnaires for the diagnosis of attention deficit hyperactivity disorder** | <ul><li>Random Forest (RF)</li><li>Decision Tree (DT)</li><li>Support Vector Machine (SVM)</li><li>Linear Discriminant Analysis (LDA)</li><li>K-nearest neighbors (KNN)</li><li>Gaussian Naïve Bayes</li><li>Logistic Regression (LR)</li><li>Artificial Neural Network (ANN)</li><li>Lasso Regression</li><li>Gradient Boosting (GB)</li><li>Elastic Net (ENet)</li><li>Q-learning</li><li>Principal Components Regression (PCR)</li></ul> | <ul><li>Psychometric questionnaires: ADHD-RS, SNAP-IV, CAARS, DIVA</li><li>Participants: Children and adults with ADHD + control groups</li><li>Sample sizes: 35 to 13,000+ participants</li></ul> | <ul><li>Accuracy</li><li>Sensitivity</li><li>Specificity</li><li>AUC</li><li>Cross-validation (k-fold, LOOCV)</li></ul> | <ul><li>**Accuracy:** 35% to 100% (most >80%)</li><li>**AUC:** 0.56 to 0.992</li><li>**Sensitivity:** 32%-100%</li><li>**Specificity:** 50%-100%</li></ul> | <ol><li>Subjectivity in parent/teacher assessments</li><li>Limited generalizability of datasets</li><li>Risk of models detecting outliers rather than pathology</li><li>Performance drop with new data</li><li>Low interpretability for clinicians</li></ol> | <ol><li>Increase ML applications</li><li>Improve interpretability</li><li>Reduce assessment time</li><li>Combine multi-informant data</li><li>Develop subtype-specific models</li></ol> |
