# Systematic Literature Review (SLR) Summary: AI-Powered Attention Assessment

| Paper Title                                                                       | AI Techniques Used                        | Dataset Description                                                                                                                                           | Evaluation Metrics               | Reported Accuracy / Effectiveness                                         | Key Challenges Noted                                                                                  | Future Directions Suggested                                                                                       |
|-----------------------------------------------------------------------------------|-------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| *Real-Time Attention Monitoring System for Classroom: A Deep Learning Approach*   | YOLOv5 (v5n, v5s, v5m, v5l, v5x), DeepSORT | - **Action Dataset**: 5,701 images (self-collected + web-sourced) labeled into 9 behaviors. <br> - **Emotion Dataset**: 35,000 images (AffectNet + masked variants). | Precision, Recall, mAP@0.5, F1-score | - **Action Detection**: 76% mAP@0.5 <br> - **Emotion Recognition**: 87.7% mAP@0.5 | - Small dataset size <br> - Limited real-world validation <br> - Privacy concerns <br> - Computational cost | - Integration with social robots <br> - Larger-scale trials <br> - Multi-modal fusion (EEG + vision) <br> - Explainable AI (XAI) |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| *Student-Engagement Detection in Classroom Using Machine Learning Algorithm* | - CATBoost<br>- XGBoost<br>- LightGBM<br>- Random Forest<br>- Multilayer Perceptron | Open University Learning Analytics Dataset (OULAD):<br>- 32,593 student records<br>- 13 features (demographics, VLE interactions, etc.)<br>- Binary engagement classification (high/low) | - Accuracy<br>- Precision<br>- Recall<br>- F1-score<br>- AUC-ROC<br>- Log Loss | - CATBoost: 92.23% accuracy, 94.64% precision, 93.3% recall, 0.9626 AUC<br>- LightGBM: 92.22% accuracy, 94.4% precision<br>- XGBoost: 92.1% accuracy<br>- Outperformed AISAR model (91% precision) | - Class imbalance (72% low engagement)<br>- Limited interpretability of complex models<br>- Generalizability across different learning environments<br>- Privacy concerns with student data | - Incorporate additional data sources (learning preferences, material characteristics)<br>- Investigate dropout-engagement correlation<br>- Adaptive learning interventions<br>- Explainable AI for model transparency |


| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| *Dyslexia Adaptive Learning Model: Student Engagement Prediction Using Machine Learning Approach* | - Bag of Features (BOF) model<br>- SURF keypoint descriptor<br>- k-Means clustering<br>- SVM (Linear/RBF kernels)<br>- Naïve Bayes<br>- k-NN | - 600 frontal face images (300 engaged/300 disengaged)<br>- Collected from 30 dyslexic students (7-12 years old)<br>- Labeled by dyslexia education experts | - Accuracy<br>- Confusion matrix (TP/TN/FP/FN) | - 97.8% accuracy (SVM Linear)<br>- 97.3% accuracy (Naïve Bayes)<br>- 97.1% accuracy (SVM RBF)<br>- k-NN performed poorly (~60-77%) | - Limited gesture variations in dataset<br>- Partial face occlusion challenges<br>- Small sample size (30 students)<br>- Difficulty classifying ambiguous states (e.g., yawning) | - Integration with adaptive learning systems for dyslexia<br>- Expansion to dynamic video analysis<br>- Inclusion of more behavioral cues (e.g., head pose)<br>- Larger-scale validation across diverse dyslexic populations |

### 17 no PDF found

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|-------------|--------------------|---------------------|--------------------|-----------------------------------|----------------------|-----------------------------|
| Machine Learning in ADHD and Depression Mental Health Diagnosis: A Survey | **SVM, CNN, Neural Networks, Random Forest, k-NN, Decision Trees, Hybrid Models** | **ADHD-200**: fMRI/EEG (973 participants)<br>**DAIC-WOZ**: Audio/video interviews (142 subjects)<br>**AVEC**: Depression datasets<br>EEG datasets (various sizes) | Accuracy<br>Sensitivity<br>Specificity<br>RMSE<br>AUC | **ADHD**:<br>- 99.58% (EEG)<br>- 97.6% (fMRI)<br>**Depression**:<br>- 100% (EEG)<br>- 89% (clinical notes) | - Small datasets<br>- Data imbalance<br>- Privacy concerns<br>- Generalizability<br>- Subjectivity in labels | - Larger multimodal datasets<br>- Real-time applications<br>- Explainable AI<br>- Clinical workflow integration |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|--------------------|---------------------|--------------------|----------------------------------|----------------------|-----------------------------|
| Machine learning in attention-deficit/hyperactivity disorder: new approaches toward understanding the neural mechanisms | SVM, Random Forest, LDA, Deep Neural Networks, Gaussian Process Classifier, LASSO Regression, Elastic Net | ADHD-200 dataset, ABCD dataset, EEG data, fMRI, sMRI, genetic data | Accuracy, AUC, Sensitivity, Specificity, Mean Square Error, Correlation | 60-90% accuracy for classification models; AUC 60-90% | Small sample sizes, interpretability limitations, generalization issues, feature selection bias | Generative models, dimensional approaches, multi-modal data integration, large-scale datasets |

| Paper Title | AI Techniques Used | Dataset Description | Evaluation Metrics | Reported Accuracy / Effectiveness | Key Challenges Noted | Future Directions Suggested |
|------------|--------------------|---------------------|--------------------|----------------------------------|----------------------|-----------------------------|
| Automatic Diagnosis of Attention Deficit Hyperactivity Disorder Using Machine Learning | Decision Tree, Random Forest, SVM, Logistic Regression, Naive Bayes, KNN | NHS clinical data (69 patients) including: <br>- Conner's ADHD Rating Scales <br>- QBTest results <br>- DIVA diagnostic interviews <br>- Risk assessment data <br>- Medical notes | Accuracy, AUC | Best model (Decision Tree): <br>- 85.5% accuracy <br>- 0.871 AUC | - Small sample size (n=69) <br>- Overfitting with medical notes <br>- Interpretability vs accuracy tradeoff <br>- High misclassification cost | - Collect larger datasets <br>- Develop fuzzy rule-based models <br>- Create clinical decision support tool <br>- Implement confidence scoring <br>- Advanced feature selection methods |
